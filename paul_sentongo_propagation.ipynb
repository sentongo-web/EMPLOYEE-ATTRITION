{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input features and target output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.5, 0.8])\n",
    "y = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([[0.2, -0.3], [0.4, 0.1], [-0.5, 0.2]])\n",
    "b1 = np.array([0.1, -0.2, 0.1])\n",
    "W2 = np.array([0.3, -0.4, 0.5])\n",
    "b2 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer input (z1): [-0.04  0.08  0.01]\n",
      "Hidden layer activations (a1): [0.   0.08 0.01]\n",
      "Output layer input (z2): 0.07300000000000001\n",
      "Output (y_hat): 0.5182418997957381\n"
     ]
    }
   ],
   "source": [
    "# The input to the hidden layer\n",
    "z1 = np.dot(W1, x) + b1\n",
    "\n",
    "# Applying the ReLU activation function\n",
    "a1 = relu(z1)\n",
    "\n",
    "# The input to the output layer\n",
    "z2 = np.dot(W2, a1) + b2\n",
    "\n",
    "# Applying sigmoid activation function\n",
    "y_hat = sigmoid(z2)\n",
    "\n",
    "# Printing out the outputs\n",
    "print(f\"Hidden layer input (z1): {z1}\")\n",
    "print(f\"Hidden layer activations (a1): {a1}\")\n",
    "print(f\"Output layer input (z2): {z2}\")\n",
    "print(f\"Output (y_hat): {y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculat the loss:\n",
    "##### - compute the binary cross-entropy loss between the predicted output and the target output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary cross-entropy loss: 0.6573131577049656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Binary cross-entropy loss function\n",
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Computing the binary cross-entropy loss\n",
    "loss = binary_cross_entropy_loss(y, y_hat)\n",
    "\n",
    "print(f\"Binary cross-entropy loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backward pass\n",
    "##### - compute the gradients of the loss with respect to the output layer weights and biases\n",
    "##### -compute the gradients of the loss with respect to the hidden layer weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss with respect to W2: [-0.         -0.03854065 -0.00481758]\n",
      "Gradient of loss with respect to b2: -0.4817581002042619\n",
      "Gradient of loss with respect to W1: [[-0.         -0.        ]\n",
      " [ 0.09635162  0.15416259]\n",
      " [-0.12043953 -0.19270324]]\n",
      "Gradient of loss with respect to b1: [-0.          0.19270324 -0.24087905]\n"
     ]
    }
   ],
   "source": [
    "# Functions for derivatives of the Sigmoid activation functions\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Binary cross-entropy loss function\n",
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Backward pass\n",
    "# Gradient of the loss with respect to z2\n",
    "dL_dz2 = y_hat - y\n",
    "\n",
    "# Gradient of the loss with respect to W2 and b2\n",
    "dL_dW2 = dL_dz2 * a1\n",
    "dL_db2 = dL_dz2\n",
    "\n",
    "# Gradient of the loss with respect to a1\n",
    "dL_da1 = dL_dz2 * W2\n",
    "\n",
    "# Gradient of the loss with respect to z1\n",
    "dL_dz1 = dL_da1 * relu_derivative(z1)\n",
    "\n",
    "# Gradient of the loss with respect to W1 and b1\n",
    "dL_dW1 = np.outer(dL_dz1, x)\n",
    "dL_db1 = dL_dz1\n",
    "\n",
    "print(f\"Gradient of loss with respect to W2: {dL_dW2}\")\n",
    "print(f\"Gradient of loss with respect to b2: {dL_db2}\")\n",
    "print(f\"Gradient of loss with respect to W1: {dL_dW1}\")\n",
    "print(f\"Gradient of loss with respect to b1: {dL_db1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the weights and biases:\n",
    "#### - Use the gradieants computed backward pass to update the weights and biases of both layers using gradient descent with a learning rate of 0.1\n",
    "\n",
    "#### After completing these steps provide the updated weights and biases of both layers and discuss the implications of the updates on the network's perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights and biases after one iteration:\n",
      "W1: [[ 0.2        -0.3       ]\n",
      " [ 0.39036484  0.08458374]\n",
      " [-0.48795605  0.21927032]]\n",
      "b1: [ 0.1        -0.21927032  0.12408791]\n",
      "W2: [ 0.3        -0.39614594  0.50048176]\n",
      "b2: 0.1481758100204262\n",
      "New hidden layer input (z1): [-0.04        0.04357909  0.05552614]\n",
      "New hidden layer activations (a1): [0.         0.04357909 0.05552614]\n",
      "New output layer input (z2): 0.15870195200089976\n",
      "New output (y_hat): 0.5395924239414079\n",
      "New binary cross-entropy loss: 0.6169411948853357\n"
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Update weights and biases\n",
    "W1 -= learning_rate * dL_dW1\n",
    "b1 -= learning_rate * dL_db1\n",
    "W2 -= learning_rate * dL_dW2\n",
    "b2 -= learning_rate * dL_db2\n",
    "\n",
    "print(\"Updated weights and biases after one iteration:\")\n",
    "print(f\"W1: {W1}\")\n",
    "print(f\"b1: {b1}\")\n",
    "print(f\"W2: {W2}\")\n",
    "print(f\"b2: {b2}\")\n",
    "\n",
    "# Forward pass with updated weights and biases to check new output and loss\n",
    "z1_new = np.dot(W1, x) + b1\n",
    "a1_new = relu(z1_new)\n",
    "z2_new = np.dot(W2, a1_new) + b2\n",
    "y_hat_new = sigmoid(z2_new)\n",
    "new_loss = binary_cross_entropy_loss(y, y_hat_new)\n",
    "\n",
    "print(f\"New hidden layer input (z1): {z1_new}\")\n",
    "print(f\"New hidden layer activations (a1): {a1_new}\")\n",
    "print(f\"New output layer input (z2): {z2_new}\")\n",
    "print(f\"New output (y_hat): {y_hat_new}\")\n",
    "print(f\"New binary cross-entropy loss: {new_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
